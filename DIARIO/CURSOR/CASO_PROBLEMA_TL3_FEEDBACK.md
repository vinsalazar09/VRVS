# üéØ CASO-PROBLEMA: TL-3 FEEDBACK ALGORITMO

**Data:** 21 de Dezembro de 2024  
**Status:** Documento para discuss√£o t√©cnica (Opus + Cursor + Usu√°rio)  
**Objetivo:** Definir algoritmo robusto para calcular feedback 0-100% a partir de TL-3 (Treino Livre com notas)

---

## 1) CONTEXTO DO VRVS

### TL-3: Treino Livre com Notas

**O que √©:**
- Extens√£o do TL-2 (Treino Livre Runner READ-ONLY)
- Usu√°rio revisa cards do Di√°rio em modo "treino livre"
- Para cada card, usu√°rio atribui nota: **0 (esqueci)**, **1 (lembrei)**, **2 (f√°cil)**
- Usu√°rio pode **pular** cards (sem atribuir nota)
- Ao finalizar treino, sistema calcula um **feedback 0-100%**

**Caracter√≠sticas cr√≠ticas:**
- ‚úÖ **Ef√™mero:** Notas do TL-3 N√ÉO persistem em `localStorage`
- ‚úÖ **N√£o altera SRS:** TL-3 n√£o modifica `srs.estagio`, `srs.proximaRevisao`, `srs.repeticoes`
- ‚úÖ **N√£o salva hist√≥rico:** TL-3 n√£o cria entrada em `window.diario.historicoRespostas`
- ‚úÖ **Fonte √∫nica:** Usa `window.treinoLivreFila` (montada pelo TL-1)

**Problema central:**
- Usu√°rio quer usar o resultado do TL-3 como **"Feedback oficial 0-100%"** na aba Feedback
- Esse feedback √© registrado manualmente na aba Feedback (campo `feedbackRendimento`)
- O feedback registrado **afeta algoritmo/ordem de revis√£o** (alto impacto no sistema)

---

## 2) O CONFLITO CENTRAL (PROBLEMA INTELECTUAL)

### 2.1 Mistura de Cards Novos vs Revisados

**Problema:**
- `window.treinoLivreFila` pode conter:
  - Cards **novos** (nunca revisados): `srs.repeticoes === 0` e `!srs.ultimaResposta`
  - Cards **revisados** (j√° tiveram pelo menos 1 revis√£o): `srs.repeticoes > 0` ou `!!srs.ultimaResposta`

**Por que √© problema:**
- Cards novos t√™m reten√ß√£o esperada **baixa** (40-50% no est√°gio 0)
- Cards revisados t√™m reten√ß√£o esperada **alta** (70-95% dependendo do est√°gio)
- **M√©dia simples** mistura ambos e distorce resultado:
  - Exemplo: 10 novos (40%) + 10 revisados (90%) = m√©dia 65%
  - Mas usu√°rio pode ter feito muito bem nos revisados (95%) e mal nos novos (30%)
  - M√©dia simples n√£o reflete desempenho real

**Impacto:**
- Feedback oficial pode ser **subestimado** (se muitos novos) ou **superestimado** (se poucos novos)
- Algoritmo de revis√£o recebe sinal errado
- Usu√°rio perde confian√ßa no sistema

---

### 2.2 Pulos/Sem Nota

**Problema:**
- Usu√°rio pode **pular** cards (sem atribuir nota)
- Pulos podem ser:
  - Cards dif√≠ceis que usu√°rio quer evitar (gaming)
  - Cards que usu√°rio j√° sabe muito bem (n√£o precisa avaliar)
  - Cards que usu√°rio n√£o tem tempo de revisar

**Por que √© problema:**
- Se pulos n√£o s√£o contabilizados, usu√°rio pode:
  - Pular todos os dif√≠ceis ‚Üí inflar feedback artificialmente
  - Pular todos os f√°ceis ‚Üí deflacionar feedback artificialmente
- Se pulos s√£o tratados como 0 (erro), penaliza usu√°rio injustamente
- Se pulos s√£o ignorados completamente, reduz amostra (n pequeno)

**Impacto:**
- Risco de **manipula√ß√£o** (gaming do sistema)
- Feedback pode n√£o refletir desempenho real
- Sistema perde confiabilidade

---

### 2.3 Amostra Pequena (n pequeno)

**Problema:**
- TL-3 pode ter **n pequeno** (ex: 3, 5, 10 cards)
- Com n pequeno:
  - M√©dia simples oscila muito (instabilidade)
  - Um √∫nico erro ou acerto muda resultado drasticamente
  - Feedback n√£o √© confi√°vel

**Exemplo num√©rico:**
- n=3: notas [2, 2, 0] ‚Üí m√©dia 66.7%
- n=3: notas [2, 2, 1] ‚Üí m√©dia 83.3%
- Diferen√ßa de 16.6% por causa de 1 card!

**Por que √© problema:**
- Feedback oficial precisa ser **est√°vel** e **confi√°vel**
- Oscila√ß√£o grande descredibiliza sistema
- Usu√°rio n√£o confia em n√∫mero que muda muito

**Impacto:**
- Feedback oscila muito com n pequeno
- Sistema perde credibilidade
- Usu√°rio n√£o usa feedback oficial

---

### 2.4 Risco de Manipula√ß√£o (Gaming)

**Problema:**
- Usu√°rio pode **manipular** feedback pulando cards dif√≠ceis
- Exemplo:
  - 20 cards: 10 f√°ceis (nota 2), 10 dif√≠ceis (nota 0)
  - M√©dia real: 50%
  - Se pular 10 dif√≠ceis: m√©dia 100% (manipula√ß√£o)

**Por que √© problema:**
- Feedback oficial deve refletir **desempenho real**
- Manipula√ß√£o distorce algoritmo de revis√£o
- Sistema perde integridade

**Impacto:**
- Feedback inflado artificialmente
- Algoritmo de revis√£o recebe sinal errado
- Sistema perde confiabilidade

---

### 2.5 Risco de Punir Cria√ß√£o de Cards Novos

**Problema:**
- Se feedback oficial considera cards novos, usu√°rio pode ser **punido** por criar conte√∫do novo
- Exemplo:
  - Usu√°rio cria 20 cards novos
  - Faz TL-3 com esses 20 cards novos
  - Reten√ß√£o esperada de novos: 40-50%
  - Feedback oficial: 45%
  - Usu√°rio √© "punido" por criar conte√∫do novo

**Por que √© problema:**
- Sistema deve **incentivar** cria√ß√£o de conte√∫do novo
- Punir cria√ß√£o desincentiva uso do sistema
- Feedback deve refletir **aprendizado**, n√£o volume

**Impacto:**
- Usu√°rio evita criar cards novos
- Sistema perde utilidade
- Feedback n√£o reflete aprendizado real

---

### 2.6 Por Que C√°lculo Ing√™nuo (M√©dia Simples) Quebra

**C√°lculo ing√™nuo:**
```javascript
const notas = [2, 2, 1, 0, 2, 1, 2, 0, 1, 2];
const media = notas.reduce((a, b) => a + b, 0) / notas.length;
const feedback = (media / 2) * 100; // 0-2 ‚Üí 0-100%
```

**Problemas:**
1. **N√£o separa novos vs revisados:** Mistura reten√ß√£o esperada diferente
2. **N√£o trata pulos:** Ignora ou penaliza injustamente
3. **N√£o estabiliza n pequeno:** Oscila muito com poucos cards
4. **N√£o previne gaming:** Permite manipula√ß√£o por pulos
5. **Pune cria√ß√£o:** Cards novos derrubam feedback

**Resultado:** Feedback n√£o √© confi√°vel, n√£o reflete desempenho real, n√£o √© √∫til para algoritmo de revis√£o.

---

## 3) REQUISITOS (CRIT√âRIOS DE QUALIDADE)

### 3.1 "Justo": N√£o Punir Cria√ß√£o de Cards Novos

**Crit√©rio:**
- Cards novos **n√£o devem derrubar** feedback oficial
- Feedback deve refletir **aprendizado**, n√£o volume de conte√∫do novo
- Sistema deve **incentivar** cria√ß√£o de conte√∫do novo

**Valida√ß√£o:**
- Adicionar 20 cards novos ‚Üí feedback n√£o deve cair
- Feedback deve ser baseado em **cards revisados** (j√° tiveram pelo menos 1 revis√£o)

---

### 3.2 "Est√°vel": N√£o Oscilar com n Pequeno

**Crit√©rio:**
- Feedback deve ser **est√°vel** mesmo com n pequeno (3-5 cards)
- Oscila√ß√£o m√°xima aceit√°vel: ¬±5% com n=3
- Sistema deve usar **shrinkage/Bayes** para estabilizar

**Valida√ß√£o:**
- n=3: feedback n√£o deve oscilar mais que ¬±5% entre execu√ß√µes similares
- n=10: feedback deve ser mais est√°vel que n=3

---

### 3.3 "Antifraude": Pulo N√£o Pode Inflar Demais

**Crit√©rio:**
- Pulos devem ter **penalidade leve** ou **cobertura m√≠nima**
- Feedback n√£o deve ser inflado artificialmente por pulos
- Sistema deve detectar e prevenir manipula√ß√£o

**Valida√ß√£o:**
- 20 cards: 10 f√°ceis (nota 2), 10 dif√≠ceis (pulados) ‚Üí feedback n√£o deve ser 100%
- Cobertura m√≠nima: pelo menos 70% dos cards devem ter nota

---

### 3.4 "Compreens√≠vel": Usu√°rio Entende e Confia

**Crit√©rio:**
- Feedback deve ser **compreens√≠vel** para usu√°rio
- Usu√°rio deve entender **como** feedback foi calculado
- Sistema deve mostrar **breakdown** (reten√ß√£o revisados, cobertura, etc.)

**Valida√ß√£o:**
- Usu√°rio consegue explicar feedback para outra pessoa
- Sistema mostra m√©tricas auxiliares (reten√ß√£o revisados, cobertura, etc.)

---

### 3.5 "Compat√≠vel": Funciona com Legado de Dados

**Crit√©rio:**
- Crit√©rio de "card revisado" deve ser **robusto a legado**
- Deve funcionar com dados antigos (sem `repeticoes` ou `ultimaResposta`)
- Sistema deve ser **retrocompat√≠vel**

**Valida√ß√£o:**
- Dados legados (sem `repeticoes`) ‚Üí sistema funciona
- Dados novos (com `repeticoes`) ‚Üí sistema funciona
- Crit√©rio robusto: `(repeticoes || 0) > 0 || !!ultimaResposta`

---

## 4) PROPOSTA A (BASEADA NA IDEIA DO CHAT)

### 4.1 Separa√ß√£o: Reten√ß√£o (Revisados) vs Aprendizado (Novos)

**Conceito:**
- **Reten√ß√£o (revisados):** Feedback baseado em cards j√° revisados (j√° tiveram pelo menos 1 revis√£o)
- **Aprendizado (novos):** Feedback baseado em cards novos (nunca revisados)
- **Feedback oficial:** Baseado **principalmente** em reten√ß√£o (revisados), com ajuste leve por aprendizado (novos)

**Crit√©rio de "card revisado":**
```javascript
const isRevisado = (entrada) => {
    const srs = entrada.srs;
    return (srs.repeticoes || 0) > 0 || !!srs.ultimaResposta;
};
```

**Crit√©rio de "card novo":**
```javascript
const isNovo = (entrada) => {
    const srs = entrada.srs;
    return (srs.repeticoes || 0) === 0 && !srs.ultimaResposta;
};
```

---

### 4.2 Mapeamento de Notas: 0/1/2 ‚Üí 0/0.5/1

**Mapeamento:**
- Nota **0 (esqueci)** ‚Üí Score **0.0** (0% de reten√ß√£o)
- Nota **1 (lembrei)** ‚Üí Score **0.5** (50% de reten√ß√£o)
- Nota **2 (f√°cil)** ‚Üí Score **1.0** (100% de reten√ß√£o)

**Justificativa:**
- Nota 1 (lembrei) indica reten√ß√£o **parcial** (lembrou, mas precisou pensar)
- Nota 2 (f√°cil) indica reten√ß√£o **total** (lembrou imediatamente)
- Nota 0 (esqueci) indica reten√ß√£o **zero** (n√£o lembrou)

---

### 4.3 Shrinkage/Bayes Simples para Estabilidade

**Conceito:**
- Usar **prior mean** e **prior n** para estabilizar com n pequeno
- F√≥rmula: `feedback = (n * media + priorN * priorMean) / (n + priorN)`

**Par√¢metros sugeridos:**
- **PriorMean:** 0.70 (70% de reten√ß√£o esperada)
- **PriorN:** 5 (equivalente a 5 observa√ß√µes)

**Justificativa:**
- PriorMean 70% reflete reten√ß√£o esperada de cards revisados (est√°gio m√©dio)
- PriorN 5 estabiliza com n pequeno (3-5 cards) sem dominar com n grande (20+ cards)

**Exemplo:**
- n=3, m√©dia=0.67 (66.7%): `feedback = (3 * 0.67 + 5 * 0.70) / (3 + 5) = 0.689 = 68.9%`
- Sem shrinkage: 66.7%
- Com shrinkage: 68.9% (mais est√°vel)

---

### 4.4 Cobertura + Penalidade Leve para Pulos

**Conceito:**
- **Cobertura:** Percentual de cards com nota (n√£o pulados)
- **Penalidade leve:** Reduzir feedback proporcionalmente √† falta de cobertura

**F√≥rmula:**
```javascript
const cobertura = cardsComNota / totalCards;
const penalidade = Math.max(0, 1 - (1 - cobertura) * 0.3); // M√°x 30% de penalidade
const feedbackAjustado = feedback * penalidade;
```

**Justificativa:**
- Cobertura 100% ‚Üí sem penalidade
- Cobertura 70% ‚Üí penalidade 9% (30% de falta * 30% de penalidade)
- Cobertura 50% ‚Üí penalidade 15% (50% de falta * 30% de penalidade)

**Exemplo:**
- Feedback: 80%
- Cobertura: 70% (7 de 10 cards com nota)
- Penalidade: 9%
- Feedback ajustado: 80% * 0.91 = 72.8%

---

### 4.5 Resultado Final: Sugest√£o de Feedback Oficial 0-100%

**Algoritmo completo (Proposta A):**

```javascript
function calcularFeedbackTL3(fila, notas) {
    // 1. Separar cards revisados vs novos
    const revisados = fila.filter((e, i) => {
        const entrada = e;
        const srs = entrada.srs;
        return (srs.repeticoes || 0) > 0 || !!srs.ultimaResposta;
    });
    const novos = fila.filter((e, i) => {
        const entrada = e;
        const srs = entrada.srs;
        return (srs.repeticoes || 0) === 0 && !srs.ultimaResposta;
    });
    
    // 2. Mapear notas para scores (0/1/2 ‚Üí 0/0.5/1)
    const scoresRevisados = revisados.map((e, i) => {
        const nota = notas[i];
        if (nota === 0) return 0.0;
        if (nota === 1) return 0.5;
        if (nota === 2) return 1.0;
        return null; // pulado
    }).filter(s => s !== null);
    
    // 3. Calcular m√©dia de reten√ß√£o (revisados)
    const nRevisados = scoresRevisados.length;
    if (nRevisados === 0) {
        // Se n√£o h√° revisados, usar apenas novos (com ajuste)
        const scoresNovos = novos.map((e, i) => {
            const nota = notas[revisados.length + i];
            if (nota === 0) return 0.0;
            if (nota === 1) return 0.5;
            if (nota === 2) return 1.0;
            return null;
        }).filter(s => s !== null);
        
        if (scoresNovos.length === 0) return null; // Sem dados
        
        const mediaNovos = scoresNovos.reduce((a, b) => a + b, 0) / scoresNovos.length;
        // Ajuste: novos t√™m reten√ß√£o esperada menor, ent√£o ajustar para cima
        const mediaAjustada = Math.min(1.0, mediaNovos * 1.2); // +20% de ajuste
        
        // Shrinkage
        const priorMean = 0.70;
        const priorN = 5;
        const feedback = (scoresNovos.length * mediaAjustada + priorN * priorMean) / (scoresNovos.length + priorN);
        
        // Cobertura
        const cobertura = scoresNovos.length / fila.length;
        const penalidade = Math.max(0, 1 - (1 - cobertura) * 0.3);
        
        return Math.round(feedback * penalidade * 100);
    }
    
    const mediaRevisados = scoresRevisados.reduce((a, b) => a + b, 0) / nRevisados;
    
    // 4. Shrinkage para estabilidade
    const priorMean = 0.70;
    const priorN = 5;
    const feedback = (nRevisados * mediaRevisados + priorN * priorMean) / (nRevisados + priorN);
    
    // 5. Cobertura + penalidade leve
    const cobertura = scoresRevisados.length / revisados.length;
    const penalidade = Math.max(0, 1 - (1 - cobertura) * 0.3);
    
    // 6. Resultado final (0-100%)
    return Math.round(feedback * penalidade * 100);
}
```

**Breakdown mostrado ao usu√°rio:**
- Reten√ß√£o revisados: X%
- Cobertura: Y%
- Feedback sugerido: Z%

---

## 5) PROPOSTA B (ALTERNATIVA PLaus√≠vel)

### 5.1 Tratamento Mais R√≠gido de Pulos

**Conceito:**
- Pulos s√£o tratados como **0 (erro)** se card √© revisado
- Pulos s√£o **ignorados** se card √© novo
- **Bloqueio:** N√£o permite finalizar TL-3 sem nota em pelo menos 70% dos cards revisados

**Justificativa:**
- Mais r√≠gido previne gaming
- For√ßa usu√°rio a avaliar cards revisados
- Cards novos podem ser pulados (n√£o afetam feedback)

**Algoritmo:**

```javascript
function calcularFeedbackTL3_B(fila, notas) {
    // 1. Separar revisados vs novos
    const revisados = fila.filter(e => isRevisado(e));
    const novos = fila.filter(e => isNovo(e));
    
    // 2. Verificar cobertura m√≠nima (70% dos revisados)
    const notasRevisados = revisados.map((e, i) => notas[i]);
    const coberturaRevisados = notasRevisados.filter(n => n !== null).length / revisados.length;
    
    if (coberturaRevisados < 0.70) {
        return null; // Bloquear finaliza√ß√£o
    }
    
    // 3. Tratar pulos em revisados como 0
    const scoresRevisados = notasRevisados.map(n => {
        if (n === null) return 0.0; // Pulo = erro
        if (n === 0) return 0.0;
        if (n === 1) return 0.5;
        if (n === 2) return 1.0;
        return 0.0;
    });
    
    // 4. Calcular m√©dia (sem shrinkage, mais direto)
    const media = scoresRevisados.reduce((a, b) => a + b, 0) / scoresRevisados.length;
    
    // 5. Ajuste por cobertura (penalidade mais forte)
    const penalidade = coberturaRevisados; // Penalidade linear
    
    // 6. Resultado final
    return Math.round(media * penalidade * 100);
}
```

**Pr√≥s:**
- ‚úÖ Mais simples (sem shrinkage)
- ‚úÖ Mais r√≠gido (previne gaming)
- ‚úÖ For√ßa avalia√ß√£o de revisados

**Contras:**
- ‚ùå Pulos em revisados penalizam muito (tratados como 0)
- ‚ùå Bloqueio pode frustrar usu√°rio
- ‚ùå N√£o estabiliza n pequeno (sem shrinkage)

---

### 5.2 Mediana Ponderada por Est√°gio

**Conceito:**
- Usar **mediana** em vez de m√©dia (mais robusta a outliers)
- **Ponderar** por est√°gio do card (cards em est√°gios mais altos t√™m mais peso)
- Ignorar pulos completamente (n√£o contabilizar)

**Justificativa:**
- Mediana √© mais robusta que m√©dia
- Ponderar por est√°gio reflete import√¢ncia do card
- Ignorar pulos evita penaliza√ß√£o injusta

**Algoritmo:**

```javascript
function calcularFeedbackTL3_C(fila, notas) {
    // 1. Separar revisados
    const revisados = fila.filter(e => isRevisado(e));
    
    // 2. Mapear notas para scores (ignorar pulos)
    const scoresComPeso = revisados.map((e, i) => {
        const nota = notas[i];
        if (nota === null) return null; // Ignorar pulo
        
        const srs = e.srs;
        const estagio = srs.estagio || 0;
        const peso = Math.pow(1.1, estagio); // Peso exponencial por est√°gio
        
        let score = 0;
        if (nota === 0) score = 0.0;
        if (nota === 1) score = 0.5;
        if (nota === 2) score = 1.0;
        
        return { score, peso };
    }).filter(s => s !== null);
    
    // 3. Calcular mediana ponderada
    // (implementa√ß√£o mais complexa, requer ordena√ß√£o e c√°lculo de mediana ponderada)
    
    // 4. Resultado final
    return Math.round(medianaPonderada * 100);
}
```

**Pr√≥s:**
- ‚úÖ Mediana √© mais robusta
- ‚úÖ Ponderar por est√°gio reflete import√¢ncia
- ‚úÖ Ignorar pulos evita penaliza√ß√£o

**Contras:**
- ‚ùå Implementa√ß√£o mais complexa
- ‚ùå N√£o previne gaming (pulos ignorados)
- ‚ùå N√£o estabiliza n pequeno

---

## 6) EXEMPLOS NUM√âRICOS (M√çNIMO 5)

### Exemplo 1: Muitos Novos + Poucos Revisados (N√£o Deve Derrubar Feedback)

**Cen√°rio:**
- Fila: 20 cards (15 novos, 5 revisados)
- Notas novos: [2, 2, 1, 2, 0, 1, 2, 2, 1, 2, 0, 1, 2, 2, 1] ‚Üí m√©dia 1.47 (73.5%)
- Notas revisados: [2, 2, 2, 1, 2] ‚Üí m√©dia 1.8 (90%)
- Cobertura: 100% (todos com nota)

**Proposta A:**
- Feedback baseado em revisados: 90%
- Shrinkage: `(5 * 0.90 + 5 * 0.70) / 10 = 0.80 = 80%`
- Cobertura: 100% ‚Üí sem penalidade
- **Feedback final: 80%**

**Proposta B:**
- Feedback baseado em revisados: 90%
- Cobertura: 100% ‚Üí sem penalidade
- **Feedback final: 90%**

**Valida√ß√£o:** ‚úÖ Novos n√£o derrubam feedback (baseado em revisados)

---

### Exemplo 2: Muitos Pulos (Mostrar Impacto)

**Cen√°rio:**
- Fila: 10 cards revisados
- Notas: [2, 2, null, null, 1, null, 2, null, 1, null]
- Cards com nota: 5 de 10 (50% de cobertura)
- M√©dia dos com nota: 1.6 (80%)

**Proposta A:**
- Feedback: 80%
- Shrinkage: `(5 * 0.80 + 5 * 0.70) / 10 = 0.75 = 75%`
- Cobertura: 50% ‚Üí penalidade: `1 - (1 - 0.50) * 0.30 = 0.85` (15% de penalidade)
- **Feedback final: 75% * 0.85 = 63.75% ‚âà 64%**

**Proposta B:**
- Pulos tratados como 0: [2, 2, 0, 0, 1, 0, 2, 0, 1, 0]
- M√©dia: 0.8 (40%)
- Cobertura: 50% ‚Üí penalidade: 0.50
- **Feedback final: 40% * 0.50 = 20%**

**Valida√ß√£o:** ‚úÖ Pulos t√™m impacto, mas Proposta A √© mais justa

---

### Exemplo 3: n Pequeno (3 cards) (Mostrar Shrinkage)

**Cen√°rio:**
- Fila: 3 cards revisados
- Notas: [2, 2, 0]
- M√©dia: 1.33 (66.7%)

**Proposta A:**
- Sem shrinkage: 66.7%
- Com shrinkage: `(3 * 0.667 + 5 * 0.70) / 8 = 0.689 = 68.9%`
- **Feedback final: 68.9%** (mais est√°vel)

**Proposta B:**
- Sem shrinkage: 66.7%
- **Feedback final: 66.7%** (oscila mais)

**Valida√ß√£o:** ‚úÖ Shrinkage estabiliza n pequeno

---

### Exemplo 4: Alto Desempenho com Alta Cobertura

**Cen√°rio:**
- Fila: 20 cards revisados
- Notas: [2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2]
- M√©dia: 1.85 (92.5%)
- Cobertura: 100%

**Proposta A:**
- Feedback: 92.5%
- Shrinkage: `(20 * 0.925 + 5 * 0.70) / 25 = 0.88 = 88%`
- Cobertura: 100% ‚Üí sem penalidade
- **Feedback final: 88%**

**Proposta B:**
- Feedback: 92.5%
- Cobertura: 100% ‚Üí sem penalidade
- **Feedback final: 92.5%**

**Valida√ß√£o:** ‚úÖ Alto desempenho reflete no feedback

---

### Exemplo 5: Baixo Desempenho Real em Revisados (Deve Refletir)

**Cen√°rio:**
- Fila: 10 cards revisados
- Notas: [0, 1, 0, 0, 1, 0, 1, 0, 0, 1]
- M√©dia: 0.4 (20%)
- Cobertura: 100%

**Proposta A:**
- Feedback: 20%
- Shrinkage: `(10 * 0.20 + 5 * 0.70) / 15 = 0.367 = 36.7%`
- Cobertura: 100% ‚Üí sem penalidade
- **Feedback final: 36.7%** (shrinkage puxa para cima, mas ainda reflete baixo desempenho)

**Proposta B:**
- Feedback: 20%
- Cobertura: 100% ‚Üí sem penalidade
- **Feedback final: 20%** (reflete baixo desempenho diretamente)

**Valida√ß√£o:** ‚úÖ Baixo desempenho reflete no feedback (Proposta B mais direta)

---

## 7) DECIS√ïES FINAIS QUE PRECISAM SER VALIDADAS "N√ìS TR√äS"

### 7.1 Crit√©rio de "Card Revisado"

**Sugest√£o (robusto a legado):**
```javascript
const isRevisado = (entrada) => {
    const srs = entrada.srs;
    return (srs.repeticoes || 0) > 0 || !!srs.ultimaResposta;
};
```

**Valida√ß√£o necess√°ria:**
- [ ] Confirmar que `srs.repeticoes` existe em dados legados
- [ ] Confirmar que `srs.ultimaResposta` existe em dados legados
- [ ] Testar com dados legados (sem `repeticoes`)
- [ ] Testar com dados novos (com `repeticoes`)

**Decis√£o:** ‚úÖ Usar crit√©rio robusto acima

---

### 7.2 Penalidade de Pulo (Se Existe e Qu√£o Forte)

**Op√ß√µes:**
- **Op√ß√£o A:** Penalidade leve (30% m√°ximo) por falta de cobertura
- **Op√ß√£o B:** Pulos tratados como 0 (mais r√≠gido)
- **Op√ß√£o C:** Pulos ignorados (sem penalidade)

**Sugest√£o:** Op√ß√£o A (penalidade leve 30%)

**Valida√ß√£o necess√°ria:**
- [ ] Definir percentual de penalidade (30%? 50%?)
- [ ] Definir cobertura m√≠nima aceit√°vel (70%? 80%?)
- [ ] Testar impacto de pulos no feedback

**Decis√£o:** ‚ö†Ô∏è Aguardar valida√ß√£o

---

### 7.3 PriorMean/PriorN (Ou Mecanismo Equivalente)

**Sugest√£o:**
- **PriorMean:** 0.70 (70% de reten√ß√£o esperada)
- **PriorN:** 5 (equivalente a 5 observa√ß√µes)

**Valida√ß√£o necess√°ria:**
- [ ] Confirmar que PriorMean 70% reflete reten√ß√£o esperada
- [ ] Confirmar que PriorN 5 estabiliza n pequeno sem dominar n grande
- [ ] Testar com n=3, n=5, n=10, n=20

**Decis√£o:** ‚ö†Ô∏è Aguardar valida√ß√£o

---

### 7.4 Como o Usu√°rio "Usa" Isso na Aba Feedback

**Op√ß√µes:**
- **Op√ß√£o A:** Campo sugerido (pr√©-preenchido, usu√°rio pode editar)
- **Op√ß√£o B:** Bot√£o "Usar feedback do TL-3" (copia valor)
- **Op√ß√£o C:** Texto explicativo (usu√°rio digita manualmente)

**Sugest√£o:** Op√ß√£o A (campo sugerido pr√©-preenchido)

**Valida√ß√£o necess√°ria:**
- [ ] Confirmar que campo `feedbackRendimento` aceita valor sugerido
- [ ] Confirmar que usu√°rio pode editar valor sugerido
- [ ] Testar UX de campo sugerido

**Decis√£o:** ‚ö†Ô∏è Aguardar valida√ß√£o

---

## 8) CRIT√âRIOS DE ACEITE (QUANDO SABEMOS QUE EST√Å BOM)

### 8.1 N√£o Punir Cria√ß√£o de Cards Novos

- [ ] Adicionar 20 cards novos ‚Üí feedback n√£o cai
- [ ] Feedback baseado em cards revisados (n√£o novos)
- [ ] Cards novos n√£o afetam feedback oficial

---

### 8.2 N√£o Inflar por Pulo

- [ ] 20 cards: 10 f√°ceis (nota 2), 10 dif√≠ceis (pulados) ‚Üí feedback n√£o √© 100%
- [ ] Cobertura m√≠nima: pelo menos 70% dos cards revisados com nota
- [ ] Penalidade por falta de cobertura aplicada corretamente

---

### 8.3 N√£o Oscilar com n Pequeno

- [ ] n=3: feedback n√£o oscila mais que ¬±5% entre execu√ß√µes similares
- [ ] Shrinkage aplicado corretamente
- [ ] Feedback est√°vel mesmo com n pequeno

---

### 8.4 Refletir Desempenho Real

- [ ] Alto desempenho (90%+) ‚Üí feedback alto (85%+)
- [ ] Baixo desempenho (20%-) ‚Üí feedback baixo (30%-)
- [ ] Feedback reflete desempenho real em cards revisados

---

### 8.5 Compat√≠vel com Legado

- [ ] Dados legados (sem `repeticoes`) ‚Üí sistema funciona
- [ ] Dados novos (com `repeticoes`) ‚Üí sistema funciona
- [ ] Crit√©rio robusto funciona com ambos

---

### 8.6 Compreens√≠vel para Usu√°rio

- [ ] Usu√°rio entende como feedback foi calculado
- [ ] Sistema mostra breakdown (reten√ß√£o revisados, cobertura, etc.)
- [ ] Usu√°rio confia no n√∫mero

---

## 9) OBSERVA√á√ïES FINAIS

### 9.1 N√£o Inventar Comportamento do App

**Quando precisar confirmar:**
- Nome exato do campo `repeticoes` ou `srs.repeticoes`
- Nome exato do campo `ultimaResposta` ou `srs.ultimaResposta`
- Estrutura exata de `window.treinoLivreFila`
- Como notas s√£o armazenadas durante TL-3 (em mem√≥ria? array?)

**A√ß√£o:** Indicar "confirmar no c√≥digo" quando necess√°rio

---

### 9.2 Pronto para Enviar ao Opus

**Formato:**
- ‚úÖ Texto t√©cnico-did√°tico
- ‚úÖ Exemplos num√©ricos (5+)
- ‚úÖ Edge cases documentados
- ‚úÖ Riscos identificados (gaming, instabilidade, puni√ß√£o)
- ‚úÖ Decis√µes pendentes claras

**Pr√≥ximo passo:** Enviar para Opus para discuss√£o t√©cnica

---

**Documento criado para discuss√£o t√©cnica (Opus + Cursor + Usu√°rio)**

